Agenda:
1. working with large files (s3)
2. nearest neighbour queries..(uber)

whenever we want to store large blobs/files...

- s3(proprietary) / hdfs (open source) : simple storage system, hadoop distributed file system

- not like CDN which is a caching only layer: these are sources of truth! Even cdn need these

- one drive, google drive are not the same - why?

Now these systems dont only store simple chunks of data...these are LARGE FILE SYSTEMS (LFS)...

now - things like 
- sample test cases (kB - MB)
- images (MB - GB)
- videos (higher GBs)

- loggers like appserver logs (tons and tons of data!!! - 100-200 TBs)
- example : manadated by govt etc like financial institutions...etc

Requirements to outline:

1. store EXTREMELY large files!
2. durability - can NOT be destroyed or lost or corrupted at ANY cost..
3. both read and write MUST be possible...

Read:
- which means download/stream the file on-demand or 
- perform analytics on it...

Write: create new file, edit/update and delete..

Now there is NO possible way to store thiese huge files ona single server!

- split the file into chunks, distribute these chunks across servers...

doubt: map reduce?

now we want to store this..what to do?

- compression algortihms: but not possible here - and not how this problem is solved as dependency mgiht be rpesent..between chunks depending on compression algorithm 

- we chunk these files...now how large do we chunk?

chunk size:

- lets say 1mb: implies: 50mn chunks for 50TB file..

to download one file: we download each chunk, append/concatenate/merge = collation.
- now this overhead would be present in ANY solution, no matter what we do...

BUT

~ the smaller the size of the chunk ~ the more this overhead
~ visible when we copy files in windows - smaller files need more time and overhead - like node modulkes....

~ WHY? because - PROBABLY - disk seek time !! diff files in different disjointed sectors!

also, if we chunk:

1. we need to store some related metadata...

for example. -MVP of a chunk:

{
which file?
offset / index?
server _d stored on?
}

- as chunk size gets smaller - the number of chunk increases -> metadata size increases even more!!


- but if we make them larger - 
- the chunks might get so big they cant store them on each server,
- if server is down, the whole chunk data is lost!!!!

some nomenclature in context of hdfs:

- name node: stores all the metadata - dedicated node - since so importsnt - it also has replicas (master-slave)

- uses ZK for this
- only one name node ! Even though
- rest are data nodes...
- in hdfs: 128MB: stores chunks of this size - but this is configurable...
-------------------------------------------------------
- for our name node: immediate consistency for the master slave thing
- for data nodes - which are also replicated... each data chunk is stored in multiple data nodes
- another new thing that these machines have are - rack aware algortihms..
-------------------------------------------------------
- replicas shouldnt be on the same machine...why? because if the machine is down - the whole thing is lost..
-------------------------------------------------------
- just like that - when you store servers in warehouse with each rack having same power supply, network router etc..... when we do this replication for data nodes, the replias for the data are not stored in the same rack!

will map-reduce help to get the entire file?

- no - reduce operation simply aggregates the data / compresses the data 

- if we reduce here! The massive file can’t be stored in app server

How do reads happen?
First on app server, then on hdfs

App server talks to name node to get the information about chunks and find metadata for each chunk

Now go to the chunk and stream this chunk - sequentially across all chunks

Note we are streaming here, not storing at once - what’s the difference?

on what basis we'll divide chunk size, also the moment app server receives where it'll be stored?

Writes?

Client sends continuous data stream to app server

App server gets chunk size from name node

App server upon chunk size reach will communicate about which server to write on, and write on it

Informs name node that data written

Name node/ data node replicates the data and app server clears buffer

Note - when client is sending data, it is simply written to a buffer

What happens if data node goes down while writing?

You always read write in chunks, if node fails, write fails and write to another server

Note - as a coder, you don’t write these rollback etc mechanisms, they are inbuilt in the library you are using

Big table by google for storing large amounts of data

Also, what if file is not a multiple of chunk size x nothing x the chunk will be less size

Serialisation and deserialisation?

Bit torrent behaves in a very similar to lfs 

torrent trackers are the name node
our machines are data nodes

Is spark similar to hdfs? No it’s a processing layer on top of hdfs

the only issue with large chunk size is the server may not be able to store large files and crash? i dont see any reason why a server cant handle atleast a few gbs as chunk size

How will zipping the folder helps here?

we can't update a chunk right, it has to be a new insert right because I don't see any scenarios where just a chunk is updated